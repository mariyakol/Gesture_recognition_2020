# -*- coding: utf-8 -*-
"""Motion_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Chukou_XA_iCBZUoLwTd3cgdSpp-yuB
"""

import os
from google.colab import drive

drive.mount('/content/drive')

# Base data path on google drive
base_path  = "/content/drive/My Drive/"

print(os.listdir(base_path))

debug = False

import cv2

def get_max_contour_index(contours): 
  max = 0
  maxindex = 0
  i = 0

  for cnt in contours:      
    area = cv2.contourArea(cnt)

    if (max < area):
      max = area
      maxindex = i
    i += 1          
          
  return maxindex

def print_all_contours(contours, threshold):
  for cnt in contours:      
    x, y, w, h = cv2.boundingRect(cnt)

    cropped = threshold[y:y+h, x:x+h]
    cv2_imshow(cropped)

def get_obj_detection_model():
  net = cv2.dnn.readNetFromCaffe(base_path + '/MobileNetSSD_deploy.prototxt.txt', base_path + '/MobileNetSSD_deploy.caffemodel')
  return net

import cv2
import numpy as np 

CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
	"bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
	"dog", "horse", "motorbike", "person", "pottedplant", "sheep",
	"sofa", "train", "tvmonitor"]
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

def get_person_bounding_box(image, net):
  (h, w) = image.shape[:2]
  if (debug):
    print(h)
    print(w)

  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 0.007843,	(300, 300), 127.5)

  net.setInput(blob)
  detections = net.forward()

  confidence = 0.6
  for i in np.arange(0, detections.shape[2]):
    # extract the confidence (i.e., probability) associated with the
    # prediction
    conf = detections[0, 0, i, 2]
    # filter out weak detections by ensuring the `confidence` is
    # greater than the minimum confidence
    if conf > confidence:
      # extract the index of the class label from the `detections`,
      # then compute the (x, y)-coordinates of the bounding box for
      # the object
      idx = int(detections[0, 0, i, 1])
      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
      (startX, startY, endX, endY) = box.astype("int")
      box_w = endX - startX
      box_h = endY - startY
      if (CLASSES[idx] == 'person'):
        print("Обнаружен объект 'person'")
        label = "{}: {:.2f}%".format(CLASSES[idx], conf * 100)
        #print("[INFO] {}".format(label))
        image2 = image.copy()
        cv2.rectangle(image2, (startX, startY), (endX, endY), COLORS[idx], 2)
        cv2.putText(image2, label, (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)
        cv2_imshow(image2)
        return (startX, endX, startY, endY, w, h)

from numpy import *
import numpy as np
from google.colab.patches import cv2_imshow
import cv2
import imutils

def crop_and_get_of(video_path, thresh, resize):
  optical_flows = []
  # Open vide and read first frame 
  cap = cv2.VideoCapture(video_path)
  
  ret = None
  current_frame = None
  frame = 0
  while (current_frame is None):
    if (debug):
      print('None')
    ret, current_frame = cap.read()
    frame += 1
  total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
  
  # Assign current to prev and resize prev
  previous_frame = current_frame
  previous_frame = cv2.resize(previous_frame, resize)
  net = get_obj_detection_model()

  x_min, x_max, y_min, y_max, w, h = get_person_bounding_box(previous_frame, net)
  (h, w) = previous_frame.shape[:2]
  box_w = x_max - x_min
  box_h = y_max - y_min
  
  inc_x_min, inc_x_max, inc_y_min, inc_y_max = (max(0, int(x_min - 0.5 * box_w)), min(w, int(x_max + 0.5 * box_w)), max(0, int(y_min - 0.5 * box_h)), min(h, int(y_max + 0.5 * box_h)))
  of_x_min, of_x_max, of_y_min, of_y_max = inc_x_min, inc_x_max, inc_y_min, inc_y_max
  # Get bounding box coordinates
  
  if (debug):
    print("x_min " + str(x_min))
    print("x_max " + str(x_max))
    print("x_max - x_min " + str(x_max - x_min))
    print("y_min " + str(y_min))
    print("y_max " + str(y_max))
    print("y_max - y_min " + str(y_max - y_min))
  
  cv2_imshow(previous_frame[y_min:y_max, x_min:x_max])

  hsv = zeros_like(previous_frame[of_y_min:of_y_max, of_x_min:of_x_max])
  hsv[..., 1] = 255
  of_acc = None
  a = []
  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)

  rec = False
  
  while(cap.isOpened()):    
    frame += 1
    print(str(frame) + '/' + str(total_frame))
    current_frame = cv2.resize(current_frame, resize)
    current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

    # NEW MOTION DETECTION 
    blurred_current = cv2.GaussianBlur(current_frame_gray, (11, 11), 0)
    blurred_prev = cv2.GaussianBlur(previous_frame_gray, (11, 11), 0)
    blurred_frame_diff = cv2.absdiff(blurred_current[of_y_min:of_y_max, of_x_min:of_x_max], blurred_prev[of_y_min:of_y_max, of_x_min:of_x_max])

    thr = cv2.threshold(blurred_frame_diff, 25, 255, cv2.THRESH_BINARY)[1]    
    thr = cv2.dilate(thr, None, iterations=1)

    cnts = cv2.findContours(thr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)

    sum_area = 0
    for c in cnts:
      sum_area += cv2.contourArea(c)

    sum_area2 = sum_area / (of_x_max - of_x_min) / (of_y_max - of_y_min)

    if (sum_area2 > thresh2):
      if (rec is False):
        rec = True
        start = frame
        bad = 0

      bad += 1
      cv2.fastNlMeansDenoising(previous_frame_gray, previous_frame_gray, 20, 5, 21)
      cv2.fastNlMeansDenoising(current_frame_gray, current_frame_gray, 20, 5, 21)

      if (bad < 18):
        ret, current_frame = cap.read()
        if (current_frame is None):
          break
        continue
 
      flow = cv2.calcOpticalFlowFarneback(previous_frame_gray[of_y_min:of_y_max, of_x_min:of_x_max], current_frame_gray[of_y_min:of_y_max, of_x_min:of_x_max], None, 0.5, 3, 4, 3, 5, 1.2, 0)
      
      mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])

      hsv[..., 0] = ang * 180 / pi / 2
      hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
      bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
      if of_acc is None:
        of_acc = bgr
      else:
        of_acc += bgr
    else:
      if of_acc is not None:
        r = True

        optical_flows.append(of_acc)        
        #break
        rec = False
        of_acc = None

    previous_frame = current_frame.copy()
    ret, current_frame = cap.read()
    if (current_frame is None):
      break

  cap.release()
  cv2.destroyAllWindows()
  return optical_flows, of_x_min, of_x_max, of_y_min, of_y_max, inc_x_min, inc_x_max, inc_y_min, inc_y_max, w, h


input_video_path = base_path + "/Program/Motion_Detection/Gesture/11/3.mp4"

thresh = 1.7
thresh2 = 0.007 #0.008
resize = (480, 270)

flows, x_min, x_max, y_min, y_max, inc_x_min, inc_x_max, inc_y_min, inc_y_max, w, h = crop_and_get_of(input_video_path, thresh, resize)

import tensorflow as tf 

model_path = base_path + 'model_NEW_37CLASSES_97.h5'#'/model_ZOOM_25.h5' #model_28_epochs_96_accuracy.h5
model = tf.keras.models.load_model(model_path)

model.summary()

import numpy as np

#labels = ['BOOK', 'BUY', 'NOT', 'IN', 'MOTHER', 'FINISH', 'LOOK', 'HOUSE', 'FUTURE', 'CAR', 'LIKE', 'READ', 'THAT', 'SEE', 'REALLY', 'WHO', 'J', 'V', 'Q']
labels = ['BUY', 'THAT', 'I', 'W', 'NOT', 'FINISH', 'FUTURE', 'REALLY', 'D', 'Q', 'K', 'T', 'P', 'BOOK', 'J', 'M', 'B', 'F', 'LIKE', 'A', 'READ', 'LOOK', 'SEE', 'R', 'H', 'N', 'G', 'V', 'U', 'O', 'S', 'IN', 'CAR', 'MOTHER', 'WHO', 'E', 'HOUSE']
print('Classes: ')
print('[\'BUY\', \'THAT\', \'I\', \'W\', \'NOT\', \'FINISH\', \'FUTURE\', \'REALLY\', \'D\',')
print('\'Q\', \'K\', \'T\', \'P\', \'BOOK\', \'J\', \'M\', \'B\', \'F\', \'LIKE\',')
print('\'A\', \'READ\', \'LOOK\', \'SEE\', \'R\', \'H\', \'N\', \'G\', \'V\',')
print('\'U\', \'O\', \'S\', \'IN\', \'CAR\', \'MOTHER\', \'WHO\', \'E\', \'HOUSE\']\n')

ii = 1
for flow in flows:
  print(str(ii) + ' / ' + str(len(flows)))
  ii += 1
  
  flow_resized = cv2.resize(flow, (64, 64))  
  flow_resized = flow_resized.reshape((1, 64, 64, 3))
  
  predictions = model.predict(flow_resized)

  index = np.argmax(predictions)

  if (predictions[0, index] > 0.4):
    print("Predictions: " )
    print(predictions)
    print('')
    
    res = tf.math.top_k(predictions, 5)
    
    for x in range(0, len(res.indices[0].numpy())):
      print(str(x + 1) + ' --- Value: ' + str(res.values[0][x].numpy()) + ', Class: ' + str(labels[res.indices[0][x].numpy()]))
         
    cv2_imshow(flow)
    print(flow_resized.shape)