# -*- coding: utf-8 -*-
"""WLASL_And_Full_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0z4Mridj0NhuQ5-qTjlbfhGCMZZnvQU
"""

import os
from google.colab import drive

drive.mount('/content/drive')

# Base data path on google drive
base_path  = "/content/drive/My Drive/dataset/"

print(os.listdir(base_path))

debug = True

import cv2

class Frame(object): #/content/drive/My Drive/dataset/asl-alphabet/A/A593.jpg
  def __init__(self, path):
    self.path = path

    # Get the sign name
    self.sign_name = self.path.split('/')[-2] # A

    # Get the image
    self.image = cv2.imread(path)
    self.image = cv2.resize(self.image, (128, 128))

  def get_image(self):
    return self.image
  
  def get_sign_name(self):
    return self.sign_name

  def __str__(self):
    return self.path + '\n' + self.sign_name + '\n' + str(self.image)

import glob
from random import shuffle
from numpy import *

class Image_Dataset(object):
  def __init__(self, path): 
    self.path = path
    self.all_signs = []
    self.sign_occurencies = {}
    self.sign_name_to_index = {}

    self.read_signs()

  def read_signs(self):
    index = 0 
    images_paths = glob.glob(self.path + '/*/*.*')
    i = 0
    print(len(images_paths))
    for image_path in images_paths:
      #if len(images_paths) % 50 == 0:
      # print(str(i) + ' / ' + str(len(images_paths)))
      i += 1
      sign = Frame(image_path)
      self.all_signs.append(sign)
      sign_name = sign.get_sign_name() 
      if sign_name in self.sign_occurencies:
        self.sign_occurencies[sign_name] += 1.0
      else:
        self.sign_occurencies[sign_name] = 1.0
        self.sign_name_to_index[sign_name] = index  
        index += 1

  def get_class_count(self):
    return len(self.sign_occurencies)

  def get_target_name(self):
    return self.sign_occurencies.keys()

  def shuffle(self):
    shuffle(self.all_signs)

  def get_images(self):
    images = []

    for sign in self.all_signs:
      images.append(sign.get_image())

    return array(images)

  def get_image_labels(self):
    labels = []
    
    for sign in self.all_signs:
      labels.append(self.sign_name_to_index[sign.get_sign_name()])
    
    return array(labels)

from sklearn.model_selection import train_test_split

# train + val + test = 1
def split_data(images, labels, train_size, test_size, val_size):
  X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=test_size, random_state=1) # 80 train, 20 test 
  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=1) # 70 train, 10 test

  return ((X_train, y_train), (X_test, y_test), (X_val, y_val))

import tensorflow as tf
 from tensorflow import keras
 from functools import partial

def get_model(input_shape, num_classes):
  kernel_size=(3, 3)
  pool_size=(2, 2)

  model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, kernel_size, padding='same', activation='relu', input_shape=input_shape),
  tf.keras.layers.Conv2D(32, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.Conv2D(32, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),
  tf.keras.layers.Conv2D(64, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.Conv2D(64, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.Conv2D(64, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),
  tf.keras.layers.Conv2D(128, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.Conv2D(128, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.Conv2D(128, kernel_size, padding='same', activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),

  tf.keras.layers.Flatten(),
  tf.keras.layers.Dropout(0.25),
  tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
  tf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
  ], name='model')
                                                  
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  
  return model

import zipfile 
zip_file =  "/content/drive/My Drive/dataset/NEW_47CLASSES.zip"
z = zipfile.ZipFile(zip_file, 'r')
z.extractall()
print(os.listdir())

print(os.listdir('NEW_47CLASSES/NEW_47CLASSES'))

import numpy as np

# Parameters
train_size = 0.70
test_size = 0.20
val_size = 0.10
n_epochs = 44
batch_size = 30
dataset_path = 'NEW_47CLASSES/NEW_47CLASSES' #+ '/Optical_flow_full/'

# Load dataset
dataset = Image_Dataset(dataset_path)
dataset.shuffle()

# Get images, labels and number of classes 
images = dataset.get_images()
labels = dataset.get_image_labels()
num_classes = dataset.get_class_count()

# Split dataset into train, test data
((X_train, y_train), (X_test, y_test), (X_val, y_val)) = split_data(images, labels, train_size, test_size, val_size)

# Number information
n = num_classes
train_n = len(X_train)
test_n = len(X_test)
val_n = len(X_val)

print("Total number of symbols: ", n)
print("Number of training images: " , train_n)
print("Number of testing images: ", test_n)
print("Number of validation images: ", val_n)

if debug:
  print('=========================== Data split information ===========================')
  print(X_train.shape)
  print(X_test.shape)
  print(y_train.shape)   
  print(y_test.shape)
  print(X_train.shape)
  print(y_train.shape) 
  print(X_val.shape)
  print(y_val.shape)

  print(images.shape)

# Set input_shape
numb_images, image_rows, image_cols, image_channels = images.shape 
input_shape = (image_rows, image_cols, image_channels)

# Convert a class vector (integers) to binary class matrix (for categorical_crossentropy)
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
y_val = keras.utils.to_categorical(y_val, num_classes)

# Get model
model = get_model(input_shape, num_classes)
model.summary()

# Fit
history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_data=(X_val, y_val), shuffle=True)

print(labels)
print(dataset.get_target_name())

score = model.evaluate(X_test, y_test, verbose=1)
print("=========================== Evaluation Metrics ===========================")
print('Test loss:', score[0])
print('Test accuracy Top 1:', score[1])

import matplotlib.pyplot as plt

# Sumarizing All Training Accuracies
plt.plot(history.history['accuracy'])
plt.title('model training accuracies')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['acc'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
#plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

from sklearn.metrics import classification_report
y_true = y_test
y_pred = model.predict(X_test)
print(y_pred.shape)
print(y_true.shape)
target_names = dataset.get_target_name()
print(target_names)
print(classification_report(np.where(y_true > 0)[1], np.argmax(y_pred, axis = 1), target_names=target_names))

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

model.save('/content/drive/My Drive/model_NEW_37CLASSES_97.h5')

# summarize history for loss
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy on train and val')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# model_json = model.to_json()
# with open("/content/drive/My Drive/model.json", "w") as json_file:             
#      json_file.write(model_json)

#      # serialize weights to HDF5
#      model.save_weights("/content/drive/My Drive/model_weights.h5")
#      print("Saved model to disk")